# -*- coding: utf-8 -*-
"""airbnb analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h-f8uKiO93PJd4PdwJjYai6BzaNJWGXH

# Import Dependencies
"""

import pandas as pd
from scipy.stats import f_oneway , chi2_contingency
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split , cross_val_score
from sklearn.feature_selection import SelectFromModel , VarianceThreshold
import numpy as np
from sklearn.decomposition import IncrementalPCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest, f_regression
from scipy.sparse import csr_matrix
#import polynomial
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler , OneHotEncoder
from xgboost import XGBRegressor
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
import tensorflow as tf
from tensorflow import keras
from sklearn.base import BaseEstimator, RegressorMixin

#ignore warnings
import warnings
warnings.filterwarnings('ignore')

data=pd.read_csv('/content/Airbnb_Data.csv')
data.head()

data.info()

data.isnull().sum()

"""# EDA"""

# Count the frequency of each unique value in 'property_type'
value_counts = data['property_type'].value_counts()

# Plotting using horizontal bar plot
plt.figure(figsize=(10, 6))
ax = sns.barplot(x=value_counts.values, y=value_counts.index, palette='plasma')

# Customizing the plot
plt.title('Distribution of Property Types')
plt.xlabel('Count')
plt.ylabel('Property Type')
plt.xticks(rotation=45)  # Rotate x-axis labels if needed

# Show value count for each bar
for i, v in enumerate(value_counts.values):
    ax.text(v + 10, i, str(v), color='black', va='center')

plt.show()

# Count the frequency of each unique value in 'room_type'
value_counts = data['room_type'].value_counts()

# Plotting using horizontal bar plot
plt.figure(figsize=(10, 6))
ax = sns.barplot(x=value_counts.values, y=value_counts.index, palette='plasma')

# Customizing the plot
plt.title('Distribution of room Types')
plt.xlabel('Count')
plt.ylabel('room Type')
plt.xticks(rotation=45)  # Rotate x-axis labels if needed

# Show value count for each bar
for i, v in enumerate(value_counts.values):
    ax.text(v + 10, i, str(v), color='black', va='center')

plt.show()

data['longitude'].nunique()

"""latitude', 'longitude', 'city

# Missing value imputation

By understanding the data distribution across data  we assumes that **Bathrooms** are dependent on **property_type** and **room_type** so for imputation we will do imputation based on this
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Create a grouped box plot to visualize the distribution of bathrooms for each combination of property type and room type
# This plot helps to understand how both property type and room type together influence the number of bathrooms.
plt.figure(figsize=(14, 8))
sns.boxplot(x='property_type', y='bathrooms', hue='room_type', data=data)
plt.title('Distribution of Bathrooms by Property Type and Room Type')
plt.xlabel('Property Type')
plt.ylabel('Number of Bathrooms')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Room Type')
plt.tight_layout()
plt.show()

# Impute missing 'bathrooms' based on the mean of 'property_type' and 'room_type'
data['bathrooms'] = data.groupby(['property_type', 'room_type'])['bathrooms'].transform(lambda x: x.fillna(x.mean()))

# If there are still missing values, impute them with the overall mean of 'bathrooms'
overall_mean_bathrooms = data['bathrooms'].mean()
data['bathrooms'].fillna(overall_mean_bathrooms, inplace=True)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from matplotlib_venn import venn3

# Create binary columns indicating missing values
data['is_host_has_profile_pic_null'] = data['host_has_profile_pic'].isna()
data['is_host_identity_verified_null'] = data['host_identity_verified'].isna()
data['is_host_since_null'] = data['host_since'].isna()

# Create a crosstab to count the number of missing values in each combination of columns
null_distribution = pd.crosstab([data['is_host_has_profile_pic_null'],
                                 data['is_host_identity_verified_null']],
                                data['is_host_since_null'])

# Print the count of missing value combinations
print(null_distribution)

venn_data = {
    '100': 0,  # Only 'host_has_profile_pic' null (not observed)
    '010': 0,  # Only 'host_identity_verified' null (not observed)
    '001': 0,  # Only 'host_since' null (not observed)
    '110': 0,  # 'host_has_profile_pic' & 'host_identity_verified' null (not observed alone)
    '011': 0,  # 'host_identity_verified' & 'host_since' null (not observed alone)
    '101': 0,  # 'host_has_profile_pic' & 'host_since' null (not observed alone)
    '111': null_distribution.loc[(True, True), True] if (True, True) in null_distribution.index and True in null_distribution.columns else 0 # All three columns null
}

# Add the count for when none are null, which should be the total number of rows minus the '111' case
total_rows = data.shape[0]
venn_data['000'] = total_rows - venn_data['111']

# Plot the Venn diagram
plt.figure(figsize=(8, 6))
venn3(subsets=venn_data, set_labels=('host_has_profile_pic', 'host_identity_verified', 'host_since'))
plt.title("Venn Diagram: Null Values in host_has_profile_pic, host_identity_verified, and host_since")
plt.show()

#for host_has_profile_pic and host_identity_verified we will drop as both hav same data mising
data.dropna(subset=['host_has_profile_pic', 'host_identity_verified', 'host_since'], axis=0, inplace=True)

""" **Zero Bedrooms** for "**Entire home/apt**" properties is likely a **data issue**, and imputing bedrooms with a **median bedroom count** for similar listings is a reasonable approach which we will be using"""

#plot bedrooms data as univarite
plt.figure(figsize=(10, 6))
sns.histplot(data['bedrooms'], bins=20, kde=True)
plt.title('Distribution of Bedrooms')
plt.xlabel('Number of Bedrooms')
plt.ylabel('Frequency')
plt.show()

# Check the distribution of room types and accommodates
data[data['bedrooms'] == 0][['room_type', 'accommodates', 'log_price']]

#  Impute bedrooms where bedrooms == 0 for 'Entire home/apt' or 'Private room' room types
median_bedrooms_entire_home = data[data['room_type'] == 'Entire home/apt']['bedrooms'].median()
data.loc[(data['bedrooms'] == 0) & (data['room_type'].isin(['Entire home/apt', 'Private room'])), 'bedrooms'] = median_bedrooms_entire_home

#  General imputation of missing bedrooms using property_type and room_type groups
data['bedrooms'] = data.groupby(['property_type', 'room_type'])['bedrooms'].transform(lambda x: x.fillna(x.median()))

"""now for **beds** we will imputate this with roomtype and property_type

"""

#impute bedrooms and beds on the basis of roomtype and property type
data['beds'] = data.groupby(['property_type', 'room_type'])['beds'].transform(lambda x: x.fillna(x.median()))

#plot reviews data as univarite
plt.figure(figsize=(10, 6))
sns.histplot(data['number_of_reviews'], bins=20, kde=True)
plt.title('Distribution of reviews')
plt.xlabel('Number of reviews')
plt.ylabel('Frequency')
plt.show()

from matplotlib_venn import venn2
import matplotlib.pyplot as plt

# Number of missing values
total_missing = 16722
filtered_missing = 15819
other_missing = total_missing - filtered_missing

# Create a Venn diagram to visualize the overlap
venn_data = {
    '10': filtered_missing,   # Only review_scores_rating is NaN and number_of_reviews is 0
    '01': other_missing,      # Other missing values
    '11': 0,                  # No overlap, as we're only interested in these two conditions
}

# Plotting the Venn diagram
plt.figure(figsize=(6, 6))
venn = venn2(subsets=venn_data, set_labels=('Review = 0 & Rating = NaN', 'Other Missing Values'))
plt.title('Venn Diagram: Missing Values in Review and Rating')

# Display the plot
plt.show()

# Check where 'review' is 0 and 'review_scores_rating' is NaN
nan_reviews = data[(data['number_of_reviews'] == 0) & (data['review_scores_rating'].isna())]

# Count the number of such entries
print("Number of entries where review is 0 and review_scores_rating is NaN:", nan_reviews.shape[0])

# Check where 'review' is not null and get the value of 'review'
non_null_reviews = data[data['number_of_reviews'].notna()]
#print("Review values when 'review' is not null:", non_null_reviews['number_of_reviews'].unique())

#  Impute 'review_scores_rating' to 0 when 'review' is 0
data.loc[data['number_of_reviews'] == 0, 'review_scores_rating'] = 0

#  Impute remaining NaN values in 'review_scores_rating' with the median
median_review_score = data['review_scores_rating'].median()
data['review_scores_rating'].fillna(median_review_score, inplace=True)

# Impute 'neighbourhood' by grouping by 'zipcode', 'latitude', 'longitude', 'city'
data['neighbourhood'] = data.groupby(['latitude', 'longitude', 'city'])['neighbourhood']\
    .transform(lambda x: x.fillna(x.mode()[0]) if not x.mode().empty else x.fillna(data['neighbourhood'].mode()[0]))

# Impute 'zipcode' by grouping by 'latitude', 'longitude', and 'city'
data['zipcode'] = data.groupby(['latitude', 'longitude', 'city'])['zipcode']\
    .transform(lambda x: x.fillna(x.mode()[0]) if not x.mode().empty else x.fillna(data['zipcode'].mode()[0]))

data.isnull().sum()

# Convert 'host_response_rate' to numeric, handling '%' and NaNs
data['host_response_rate'] = data['host_response_rate'].str.replace('%', '').astype(float)

# impute host_response_rate based on 'host_has_profile_pic', 'host_identity_verified', 'host_since'
# If group median is not available, use the overall median of host_response_rate
overall_median_response_rate = data['host_response_rate'].median()
data['host_response_rate'] = data.groupby(['host_has_profile_pic', 'host_identity_verified', 'host_since'])['host_response_rate'].transform(lambda x: x.fillna(x.median() if not pd.isna(x.median()) else overall_median_response_rate))

# Convert to datetime format (if not already done)
data['first_review'] = pd.to_datetime(data['first_review'], errors='coerce')
data['last_review'] = pd.to_datetime(data['last_review'], errors='coerce')

# Plot histograms for first_review and last_review
plt.figure(figsize=(14, 6))

# First Review Histogram
plt.subplot(1, 2, 1)
data['first_review'].dropna().hist(bins=50, color='skyblue', edgecolor='black')
plt.title('Distribution of First Review Dates')
plt.xlabel('First Review Date')
plt.ylabel('Frequency')

# Last Review Histogram
plt.subplot(1, 2, 2)
data['last_review'].dropna().hist(bins=50, color='lightcoral', edgecolor='black')
plt.title('Distribution of Last Review Dates')
plt.xlabel('Last Review Date')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

median_first_review = data['first_review'].median()
data['first_review'].fillna(median_first_review, inplace=True)
median_last_review = data['last_review'].median()
data['last_review'].fillna(median_last_review, inplace=True)

# Set a placeholder image URL for missing thumbnail URLs
placeholder_url = 'https://www.google.com/search?sca_esv=637c0e90f834ed3d&rlz=1C1CHBF_enPK1153PK1153&sxsrf=AE3TifM1Tr7p3wrjESCShM7PV_0bq0lFjQ:1758613022932&udm=2&fbs=AIIjpHxU7SXXniUZfeShr2fp4giZ1Y6MJ25_tmWITc7uy4KIeoJTKjrFjVxydQWqI2NcOhYPURIv2wPgv_w_sE_0Sc6QqqU7k8cSQndc5mTXCIWHa5yWh8UZLeaMB2TzsL707pc1UdUOyvWrdH9KzB0rwa56e4sZMK6yB9HCSc5sZ95qH7WhtZ4UgYYwhFKAtUJ9yDKl7bQ8&q=no+image+available&sa=X&ved=2ahUKEwiNxp2EsO6PAxWDS_EDHZ29HQUQtKgLegQIEhAB&biw=1280&bih=559&dpr=1.5#vhid=eTmuGeq2FpQqAM&vssid=mosaic'
data['thumbnail_url'].fillna(placeholder_url, inplace=True)

#CHECK Duplicate Entries
data.duplicated().sum()

data.info()

"""# Univariate analysis"""

data.describe()

"""# Feaature Selection"""

# Select only numerical columns for correlation matrix calculation
numerical_cols = data.select_dtypes(include=np.number).columns

# Correlation Matrix for Numerical Features
corr_matrix = data[numerical_cols].corr()

# Plotting the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix")
plt.show()

# Set a stricter correlation threshold, e.g., 0.85
threshold = 0.7
high_corr_var = np.where(np.abs(corr_matrix) > threshold)
high_corr_var = [(i, j) for i, j in zip(*high_corr_var) if i != j]

# Drop the columns that have high correlation
dropped_columns = []
for i, j in high_corr_var:
    colname = corr_matrix.columns[i]
    if colname not in dropped_columns:  # Ensure each column is only dropped once
        dropped_columns.append(colname)
        print(f"Dropping column due to high correlation: {colname}")
        data = data.drop(columns=[colname])

data.info()

# Convert 'log_price' to categorical (price_category) for Chi-Square
data['price_category'] = pd.qcut(data['log_price'], q=4, labels=["Low", "Medium", "High", "Very High"])

# For Chi-Square: Check p-values for categorical features
categorical_columns = data.select_dtypes(include='object').columns
significant_categorical = []
non_significant_categorical = []  # To store non-significant features

# Set a stricter p-value threshold to drop more columns
threshold = 0.0001  # Can be adjusted to 0.001 for an even stricter criterion

for feature in categorical_columns:
    if feature != 'price_category':  # Don't include the target variable
        contingency_table = pd.crosstab(data[feature], data['price_category'])
        chi2, p, dof, expected = chi2_contingency(contingency_table)
        print(f"Chi-Square Test for {feature}: p-value = {p}")

        # Collect significant and non-significant features
        if p < threshold:
            significant_categorical.append(feature)
            print(f"Feature {feature} is significant.")
        else:
            non_significant_categorical.append(feature)
            print(f"Feature {feature} is NOT significant.")

# List of significant categorical features
print(f"\nSignificant Categorical Features: {significant_categorical}")
print(f"Non-Significant Categorical Features: {non_significant_categorical}")

#drop name and thumbnail_url
data.drop(columns=['description', 'host_has_profile_pic', 'name', 'thumbnail_url'], inplace=True)

# Drop categorical columns with too many unique values (e.g., > 20 unique categories)
categorical_columns = data.select_dtypes(include='object').columns
for feature in categorical_columns:
    if data[feature].nunique() > 10:
        print(f"Dropping {feature} due to too many unique values")
        data.drop(columns=[feature], inplace=True)

data.info()

# Numerical features to check for significance
numerical_columns = data.select_dtypes(include='number').columns
significant_numerical = []
non_significant_numerical = []  # To store non-significant features

# Set a stricter p-value threshold (e.g., 0.01)
threshold = 0.0001

for feature in numerical_columns:
    if feature != 'log_price':  # Don't include the target variable
        # Group the data based on price_category and get the feature values for each group
        groups = [data[data['price_category'] == cat][feature] for cat in data['price_category'].unique()]

        # Perform the ANOVA test
        f_stat, p_value = f_oneway(*groups)
        print(f"ANOVA Test for {feature}: p-value = {p_value}")

        # Collect significant and non-significant features based on the p-value
        if p_value < threshold:
            significant_numerical.append(feature)
            print(f"Feature {feature} is significant.")
        else:
            non_significant_numerical.append(feature)
            print(f"Feature {feature} is NOT significant.")

# List of significant and non-significant numerical features
print(f"\nSignificant Numerical Features: {significant_numerical}")
print(f"Non-Significant Numerical Features: {non_significant_numerical}")

#Feature id is NOT significant ao drop it
data.drop(columns=['id', 'host_response_rate'], inplace=True)

data.info()

#drop price category column
data.drop(columns=['price_category'], inplace=True)

data.info()

"""# Model data Prepration"""

#create X and y
X=data.drop(columns=['log_price'])
y=data['log_price']

#create x and y train test split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

numerical_features=X_train.select_dtypes(include=['int64','float64']).columns
categorical_features=X_train.select_dtypes(include=['object']).columns

numerical_pipe = Pipeline([('scaler', StandardScaler())])
categorical_pipleline=Pipeline([('onehot',OneHotEncoder(handle_unknown='ignore'))])

#column transformer
preprocessor=ColumnTransformer([
    ('numerical',numerical_pipe,numerical_features),
    #varable,ftn,data
    ('cat',categorical_pipleline,categorical_features)
])

"""# Linear Regression"""

# Create a pipeline with the preprocessor and Linear Regression model
pipeline_lr = Pipeline([
    ('preprocessor', preprocessor),  # preprocessor should be defined elsewhere
    ('model', LinearRegression())
])

# Perform cross-validation
cv_scores_lr = cross_val_score(pipeline_lr, X_train, y_train, cv=5, scoring='neg_mean_squared_error')

# Print the cross-validation scores
print("Cross-validation scores (Negative Mean Squared Error):", cv_scores_lr)
print("Mean CV score:", cv_scores_lr.mean())

# Fit the model on the training set to evaluate it
pipeline_lr.fit(X_train, y_train)

# Predict on the test set
y_pred_lr = pipeline_lr.predict(X_test)
print(f"first five predictions: {y_pred_lr[:5]}\n")

# Evaluate the model on the test set using Mean Squared Error and R² Score
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

print(f"Mean Squared Error (MSE) on Test Set: {mse_lr}")
print(f"R² Score on Test Set: {r2_lr}")

"""#RandomForestRegressor"""

# Create a pipeline with the preprocessor and Random Forest Regressor model
pipeline_rf = Pipeline([
    ('preprocessor', preprocessor),
    ('model', RandomForestRegressor(random_state=42)) # Added random_state for reproducibility
])

# Perform cross-validation
cv_scores_rf = cross_val_score(pipeline_rf, X_train, y_train, cv=5, scoring='neg_mean_squared_error')

# Print the cross-validation scores
print("Cross-validation scores (Negative Mean Squared Error) for Random Forest:", cv_scores_rf)
print("Mean CV score for Random Forest:", cv_scores_rf.mean())

# Fit the model on the training set to evaluate it
pipeline_rf.fit(X_train, y_train)

# Predict on the test set
y_pred_rf = pipeline_rf.predict(X_test)
print(f"\nFirst five predictions from Random Forest: {y_pred_rf[:5]}\n")

# Evaluate the model on the test set using Mean Squared Error and R² Score
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"Mean Squared Error (MSE) on Test Set for Random Forest: {mse_rf}")
print(f"R² Score on Test Set for Random Forest: {r2_rf}")

"""# XGBRegressor"""

# Create a pipeline with the preprocessor and XGBoost Regressor model
pipeline_xgb = Pipeline([
    ('preprocessor', preprocessor),
    ('model', XGBRegressor(random_state=42)) # Added random_state for reproducibility
])

# Perform cross-validation
cv_scores_xgb = cross_val_score(pipeline_xgb, X_train, y_train, cv=5, scoring='neg_mean_squared_error')

# Print the cross-validation scores
print("Cross-validation scores (Negative Mean Squared Error) for XGBoost:", cv_scores_xgb)
print("Mean CV score for XGBoost:", cv_scores_xgb.mean())

# Fit the model on the training set to evaluate it
pipeline_xgb.fit(X_train, y_train)

# Predict on the test set
y_pred_xgb = pipeline_xgb.predict(X_test)
print(f"\nFirst five predictions from XGBoost: {y_pred_xgb[:5]}\n")

# Evaluate the model on the test set using Mean Squared Error and R² Score
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

print(f"Mean Squared Error (MSE) on Test Set for XGBoost: {mse_xgb}")
print(f"R² Score on Test Set for XGBoost: {r2_xgb}")

"""# GradientBoostingRegressor"""

# Create a pipeline with the preprocessor and Gradient Boosting Regressor model
pipeline_gbr = Pipeline([
    ('preprocessor', preprocessor),
    ('model', GradientBoostingRegressor(random_state=42)) # Added random_state for reproducibility
])

# Perform cross-validation
cv_scores_gbr = cross_val_score(pipeline_gbr, X_train, y_train, cv=5, scoring='neg_mean_squared_error')

# Print the cross-validation scores
print("Cross-validation scores (Negative Mean Squared Error) for Gradient Boosting:", cv_scores_gbr)
print("Mean CV score for Gradient Boosting:", cv_scores_gbr.mean())

# Fit the model on the training set to evaluate it
pipeline_gbr.fit(X_train, y_train)

# Predict on the test set
y_pred_gbr = pipeline_gbr.predict(X_test)
print(f"\nFirst five predictions from Gradient Boosting: {y_pred_gbr[:5]}\n")


# Evaluate the model on the test set using Mean Squared Error and R² Score
mse_gbr = mean_squared_error(y_test, y_pred_gbr)
r2_gbr = r2_score(y_test, y_pred_gbr)

print(f"Mean Squared Error (MSE) on Test Set for Gradient Boosting: {mse_gbr}")
print(f"R² Score on Test Set for Gradient Boosting: {r2_gbr}")

"""# ANN"""

# Create a wrapper for the Keras model to be used in scikit-learn pipeline
class KerasRegressorWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, build_fn, epochs=10, batch_size=32, verbose=0, **kwargs):
        self.build_fn = build_fn
        self.epochs = epochs
        self.batch_size = batch_size
        self.verbose = verbose
        self.kwargs = kwargs
        self.model = None

    def fit(self, X, y):
        # Remove input_dim from kwargs as it's passed directly
        fit_kwargs = self.kwargs.copy()
        fit_kwargs.pop('input_dim', None)

        self.model = self.build_fn(input_dim=X.shape[1], **fit_kwargs)
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)
        return self

    def predict(self, X):
        return self.model.predict(X, verbose=self.verbose).flatten()

    def score(self, X, y):
        y_pred = self.predict(X)
        return r2_score(y, y_pred)


# Function to build the Keras model
def build_two_layer_ann(input_dim):
    model = keras.Sequential([
        keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),
        keras.layers.Dense(32, activation='relu'),
        keras.layers.Dense(1) # Output layer for regression
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

# Get the input dimension after preprocessing
# We need to fit the preprocessor first to get the output shape
X_train_processed = preprocessor.fit_transform(X_train)
input_dim = X_train_processed.shape[1]

# Create the Keras Regressor wrapper
keras_regressor = KerasRegressorWrapper(build_fn=build_two_layer_ann, input_dim=input_dim, epochs=50, batch_size=64, verbose=0)

# Create a pipeline with the preprocessor and Keras model
pipeline_ann = Pipeline([
    ('preprocessor', preprocessor),
    ('model', keras_regressor)
])



# Fit the model on the training set to evaluate it
print("Training the ANN model...")
pipeline_ann.fit(X_train, y_train)
print("Training finished.")

# Predict on the test set
y_pred_ann = pipeline_ann.predict(X_test)
print(f"\nFirst five predictions from ANN: {y_pred_ann[:5]}\n")


# Evaluate the model on the test set using Mean Squared Error and R² Score
mse_ann = mean_squared_error(y_test, y_pred_ann)
r2_ann = r2_score(y_test, y_pred_ann)

print(f"Mean Squared Error (MSE) on Test Set for ANN: {mse_ann}")
print(f"R² Score on Test Set for ANN: {r2_ann}")

"""# Models evaluation"""

# Create a dictionary to store the results
results = {
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Gradient Boost', 'ANN'],
    'MSE': [mse_lr, mse_rf, mse_xgb, mse_gbr, mse_ann],
    'R2': [r2_lr, r2_rf, r2_xgb, r2_gbr, r2_ann]
}

# Create a DataFrame from the results dictionary
results_df = pd.DataFrame(results)

# Display the DataFrame
print("Model Performance Comparison:")
display(results_df)

# Plot the R² scores
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='R2', data=results_df, palette='viridis')
plt.title('R² Score Comparison of Different Models')
plt.xlabel('Model')
plt.ylabel('R² Score')
plt.ylim(0, 1) # R² score is between 0 and 1
plt.show()

# Plot the MSE scores (optional, as R² is often more intuitive)
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='MSE', data=results_df, palette='viridis')
plt.title('Mean Squared Error (MSE) Comparison of Different Models')
plt.xlabel('Model')
plt.ylabel('MSE')
plt.show()

"""#key predictor"""

# Feature Importance from Random Forest
rf_importance = pipeline_rf.named_steps['model'].feature_importances_

# Feature Importance from XGBoost
xgb_importance = pipeline_xgb.named_steps['model'].feature_importances_

# Visualizing Feature Importance
numerical_feature_names = numerical_features.tolist()
categorical_transformer = pipeline_rf.named_steps['preprocessor'].named_transformers_['cat']
onehot_feature_names = categorical_transformer.named_steps['onehot'].get_feature_names_out(categorical_features)
all_feature_names = numerical_feature_names + list(onehot_feature_names)


features = all_feature_names # Use the feature names after preprocessing

# Plotting Random Forest Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=features, y=rf_importance)
plt.xticks(rotation=90)
plt.title('Random Forest Feature Importance')
plt.show()

# Plotting XGBoost Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=features, y=xgb_importance)
plt.xticks(rotation=90)
plt.title('XGBoost Feature Importance')
plt.show()

gbr_importance = pipeline_gbr.named_steps['model'].feature_importances_


numerical_feature_names = numerical_features.tolist()

categorical_transformer = pipeline_gbr.named_steps['preprocessor'].named_transformers_['cat']
onehot_feature_names = categorical_transformer.named_steps['onehot'].get_feature_names_out(categorical_features)
all_feature_names = numerical_feature_names + list(onehot_feature_names)


features = all_feature_names # Use the feature names after preprocessing

# Plotting Gradient Boosting Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=features, y=gbr_importance)
plt.xticks(rotation=90)
plt.title('Gradient Boosting Feature Importance')
plt.show()

lr_model = pipeline_lr.named_steps['model']  # Access the Linear Regression model from the pipeline

# Getting the coefficients (Effect of Features)
coefficients = pd.DataFrame(lr_model.coef_, index=all_feature_names, columns=['Coefficient'])

# Sorting coefficients
coefficients = coefficients.sort_values(by='Coefficient', ascending=False)

# Visualizing coefficients
plt.figure(figsize=(12, 6))
sns.barplot(x=coefficients.index, y=coefficients['Coefficient'])
plt.xticks(rotation=90)
plt.title('Feature Coefficients from Linear Regression')
plt.show()

train_data = pd.concat([X_train, y_train], axis=1)

# Analyze impact of categorical features on log_price
categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns
print("Impact of Categorical Features on log_price (Mean Price per Category):")
for feature in categorical_features:
    if feature != 'log_price':  # Exclude the target variable
        print(f"\nFeature: {feature}")
        display(train_data.groupby(feature)['log_price'].mean().sort_values(ascending=False).reset_index())

# Analyze impact of numerical features on log_price (Correlation)
numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns
print("\nImpact of Numerical Features on log_price (Correlation Coefficient):")
# Exclude datetime columns from correlation and include log_price for correlation calculation
numerical_features_and_target = [col for col in numerical_features if col != 'log_price'] + ['log_price']

if numerical_features_and_target:
    # Calculate correlation on the DataFrame including numerical features and the target
    correlation_with_price = train_data[numerical_features_and_target].corr()['log_price'].sort_values(ascending=False).reset_index()
    correlation_with_price.columns = ['Feature', 'Correlation with log_price']
    # Remove the correlation of log_price with itself from the output
    correlation_with_price = correlation_with_price[correlation_with_price['Feature'] != 'log_price']
    display(correlation_with_price)
else:
    print("No numerical features available for correlation analysis.")

#check is X has property type column
if 'property_type' in X_train.columns:
  print("yes")
else:
  print("no")

# Apply the preprocessor to transform the data first
X_train_processed = preprocessor.transform(X_train)

# Create interaction terms on the processed numerical data
poly = PolynomialFeatures(interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X_train_processed)

# Fitting a regression model with interaction terms
lr_poly = LinearRegression()
lr_poly.fit(X_poly, y_train)

poly_feature_names = poly.get_feature_names_out(input_features=preprocessor.get_feature_names_out())


coefficients_poly = pd.DataFrame(lr_poly.coef_, index=poly_feature_names, columns=['Coefficient'])


top_n = 20
bottom_n = 20
sorted_coefficients = coefficients_poly.sort_values(by='Coefficient', ascending=False)
top_coefficients = sorted_coefficients.head(top_n)
bottom_coefficients = sorted_coefficients.tail(bottom_n)

# Combine top and bottom coefficients for plotting
plotting_coefficients = pd.concat([top_coefficients, bottom_coefficients])

plt.figure(figsize=(12, 8))
sns.barplot(x=plotting_coefficients.index, y=plotting_coefficients['Coefficient'])
plt.xticks(rotation=90)
plt.title('Top and Bottom Interaction Term Coefficients from Linear Regression')
plt.xlabel('Interaction Term')
plt.ylabel('Coefficient')
plt.tight_layout() # Adjust layout to prevent labels overlapping
plt.show()